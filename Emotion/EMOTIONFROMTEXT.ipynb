{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "473a0134-e8c2-459b-94c1-64330403a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detects emotions from text\n",
    "#Created by S. Biswas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed84f5c0-14ee-486e-8a34-ce5dd5939d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt', sep=';', names=['Sentence', 'Emotion'], encoding='UTF8')\n",
    "#data cleaning\n",
    "df_train = df_train[df_train['Sentence'] != '']\n",
    "df_train = df_train.dropna()\n",
    "df_train = df_train.drop_duplicates()\n",
    "# df_train = df_train[df_train['Sentence'] != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35f1b7e7-de1c-4fc0-b664-a2433cd3b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['length'] = [len(sen) for sen in df_train['Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01ecf010-c3dd-4c35-b6f0-47730926eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad4d94c7-ef3b-4047-8f7c-9cc5b891acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sea\n",
    "# sea.countplot(x=df_train['Emotion'])\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "df_train['Emotion'] = lb.fit_transform(df_train['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b7f0a10-6b83-456b-83ca-3283d41c95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee2f097f-b838-4b8e-a6f3-489e1203c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df_train['Cleaned_Sentence'] = df_train['Sentence'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "df_train['Cleaned_Sentence'] = df_train['Cleaned_Sentence'].apply(lambda x: x.lower())\n",
    "df_train['Final_Cleaned_Sentence'] = df_train['Cleaned_Sentence'].apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split() if word not in stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b7d58df-d735-4f83-a673-bea51b72345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>length</th>\n",
       "      <th>Cleaned_Sentence</th>\n",
       "      <th>Final_Cleaned_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel Humiliated</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>didnt feel humili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>go feel hopeless damn hope around someon care ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>feel grouchi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Emotion  length  \\\n",
       "0                            i didnt feel Humiliated        4      23   \n",
       "1  i can go from feeling so hopeless to so damned...        4     108   \n",
       "2   im grabbing a minute to post i feel greedy wrong        0      48   \n",
       "3  i am ever feeling nostalgic about the fireplac...        3      92   \n",
       "4                               i am feeling grouchy        0      20   \n",
       "\n",
       "                                    Cleaned_Sentence  \\\n",
       "0                            i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned...   \n",
       "2   im grabbing a minute to post i feel greedy wrong   \n",
       "3  i am ever feeling nostalgic about the fireplac...   \n",
       "4                               i am feeling grouchy   \n",
       "\n",
       "                              Final_Cleaned_Sentence  \n",
       "0                                  didnt feel humili  \n",
       "1  go feel hopeless damn hope around someon care ...  \n",
       "2               im grab minut post feel greedi wrong  \n",
       "3     ever feel nostalg fireplac know still properti  \n",
       "4                                       feel grouchi  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a1d4d6e-edb5-4f02-be6c-c9d0ce48bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['Final_Cleaned_Sentence'], df_train['Emotion'], test_size= 0.2 , random_state=42 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb2c9ff4-14d0-47dc-af31-6bf9a568af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8853)\t0.31232674165251334\n",
      "  (0, 6835)\t0.31232674165251334\n",
      "  (0, 7972)\t0.20517199765788086\n",
      "  (0, 6475)\t0.1764283970540595\n",
      "  (0, 4991)\t0.31232674165251334\n",
      "  (0, 4404)\t0.31232674165251334\n",
      "  (0, 7465)\t0.22465437747659178\n",
      "  (0, 4784)\t0.31232674165251334\n",
      "  (0, 6695)\t0.2901547609783056\n",
      "  (0, 4198)\t0.28301697747997845\n",
      "  (0, 7456)\t0.20517199765788086\n",
      "  (0, 2901)\t0.032412553141535384\n",
      "  (0, 3684)\t0.15613857137659465\n",
      "  (0, 1147)\t0.1604098943217147\n",
      "  (0, 8145)\t0.16139419987817885\n",
      "  (0, 1746)\t0.21731088647469238\n",
      "  (0, 6555)\t0.2403137555932144\n",
      "  (1, 8288)\t0.4671569474614646\n",
      "  (1, 8039)\t0.43951216075960775\n",
      "  (1, 7459)\t0.4153149781387904\n",
      "  (1, 550)\t0.5174465558324732\n",
      "  (1, 9117)\t0.3485419971324422\n",
      "  (1, 2901)\t0.16393429784069966\n",
      "  (2, 4690)\t0.40099387094537087\n",
      "  (2, 7709)\t0.35087417987605735\n",
      "  :\t:\n",
      "  (12794, 6493)\t0.4274935959265569\n",
      "  (12794, 1620)\t0.39207774264510764\n",
      "  (12794, 4598)\t0.34162795753036035\n",
      "  (12794, 7327)\t0.3778581392591926\n",
      "  (12794, 4788)\t0.2531631000602922\n",
      "  (12794, 2901)\t0.0627621999234857\n",
      "  (12795, 2022)\t0.989659889114929\n",
      "  (12795, 2901)\t0.14343397044294146\n",
      "  (12796, 7734)\t0.6570055433727381\n",
      "  (12796, 7643)\t0.5569757068084829\n",
      "  (12796, 4718)\t0.4933210763260223\n",
      "  (12796, 2901)\t0.12147466260590722\n",
      "  (12797, 6583)\t0.4558547411671889\n",
      "  (12797, 1595)\t0.4145830514439431\n",
      "  (12797, 1809)\t0.3844297897277034\n",
      "  (12797, 5322)\t0.6319517347847308\n",
      "  (12797, 45)\t0.2659548057815349\n",
      "  (12797, 2901)\t0.04935718149381211\n",
      "  (12798, 9136)\t0.5658674241411735\n",
      "  (12798, 4771)\t0.4102232694813414\n",
      "  (12798, 6830)\t0.4186033118958587\n",
      "  (12798, 7460)\t0.28673970556892575\n",
      "  (12798, 2302)\t0.37688320795282354\n",
      "  (12798, 7459)\t0.32836224123776664\n",
      "  (12798, 2901)\t0.06480603432116382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer()\n",
    "x_train_tfidf = tfidfv.fit_transform(x_train)\n",
    "x_test_tfidf = tfidfv.transform(x_test)\n",
    "print(x_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f23e9409-2471-4d73-ab2f-8bcf19d23f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====MultinomialNB======\n",
      "\n",
      "=====0.6590625======\n",
      "\n",
      "======              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.32      0.48       439\n",
      "           1       0.88      0.23      0.37       375\n",
      "           2       0.58      0.98      0.73      1027\n",
      "           3       1.00      0.03      0.05       303\n",
      "           4       0.72      0.91      0.80       950\n",
      "           5       1.00      0.02      0.04       106\n",
      "\n",
      "    accuracy                           0.66      3200\n",
      "   macro avg       0.85      0.42      0.41      3200\n",
      "weighted avg       0.76      0.66      0.59      3200\n",
      "=====\n",
      "=====SVC======\n",
      "\n",
      "=====0.815625======\n",
      "\n",
      "======              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83       439\n",
      "           1       0.84      0.73      0.78       375\n",
      "           2       0.75      0.94      0.83      1027\n",
      "           3       0.82      0.37      0.51       303\n",
      "           4       0.87      0.90      0.89       950\n",
      "           5       0.82      0.47      0.60       106\n",
      "\n",
      "    accuracy                           0.82      3200\n",
      "   macro avg       0.83      0.70      0.74      3200\n",
      "weighted avg       0.82      0.82      0.80      3200\n",
      "=====\n",
      "=====RandomForestClassifier======\n",
      "\n",
      "=====0.846875======\n",
      "\n",
      "======              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       439\n",
      "           1       0.83      0.84      0.84       375\n",
      "           2       0.84      0.90      0.87      1027\n",
      "           3       0.82      0.59      0.69       303\n",
      "           4       0.91      0.88      0.90       950\n",
      "           5       0.68      0.73      0.70       106\n",
      "\n",
      "    accuracy                           0.85      3200\n",
      "   macro avg       0.81      0.80      0.80      3200\n",
      "weighted avg       0.85      0.85      0.84      3200\n",
      "=====\n",
      "=====LogisticRegression======\n",
      "\n",
      "=====0.8240625======\n",
      "\n",
      "======              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.83       439\n",
      "           1       0.85      0.72      0.78       375\n",
      "           2       0.76      0.94      0.84      1027\n",
      "           3       0.83      0.46      0.59       303\n",
      "           4       0.88      0.92      0.90       950\n",
      "           5       0.74      0.46      0.57       106\n",
      "\n",
      "    accuracy                           0.82      3200\n",
      "   macro avg       0.82      0.71      0.75      3200\n",
      "weighted avg       0.83      0.82      0.82      3200\n",
      "=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauman/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = {\n",
    "    'MultinomialNB' : MultinomialNB(),\n",
    "    'SVC':SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(),\n",
    "    'LogisticRegression' : LogisticRegression(),   \n",
    "}\n",
    "for name, cls in classifier.items():\n",
    "    cls.fit(x_train_tfidf, y_train)\n",
    "    y_pred = cls.predict(x_test_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"====={name}======\\n\")\n",
    "    print(f\"====={acc}======\\n\")\n",
    "    rep = classification_report(y_test, y_pred)\n",
    "    print(f\"======{rep}=====\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750911c-0923-4e43-9c11-fd5adf7e3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7105effe-7c10-47df-b2a9-e462c9a7da60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauman/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming stemmer and tfidfv are already defined and trained\n",
    "lg = LogisticRegression()\n",
    "lg.fit(x_train_tfidf, y_train)\n",
    "\n",
    "def clean_data_for_predict(text):\n",
    "    clean_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    t = clean_text.lower()  # Apply lowercasing on clean_text\n",
    "\n",
    "    # Ensure stemmer is called correctly\n",
    "    t = ' '.join(stemmer.stem(w) for w in t.split())\n",
    "    return t\n",
    "\n",
    "def predict_emotion(text):\n",
    "    text = clean_data_for_predict(text)\n",
    "    v = tfidfv.transform([text])  # Wrap text in a list to form a 2D array\n",
    "    label = lg.predict(v)[0]  # Get the predicted label directly\n",
    "    emo = lb.inverse_transform(lg.predict(v))[0]\n",
    "    return emo, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef01a4aa-3090-47b3-98d1-251a9e8b6815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('joy', 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_emotion(\"I make you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81db3e-15f1-43c8-8870-4794b6503644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
